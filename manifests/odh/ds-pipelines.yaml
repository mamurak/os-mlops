kind: Secret
apiVersion: v1
metadata:
  name: mysql-secret
  namespace: redhat-ods-applications
stringData:
  password: mlpipeline
  username: mlpipeline
type: Opaque
---
kind: Secret
apiVersion: v1
metadata:
  name: mlpipeline-minio-artifact
  namespace: redhat-ods-applications
stringData:
  accesskey: minio
  host: minio-service.minio.svc.cluster.local
  port: "9000"
  secretkey: minio123
  secure: "false"
type: Opaque
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: pipeline-install-config
  namespace: redhat-ods-applications
data:
  pipelineDb: mlpipeline
  cacheDb: mlpipeline
  defaultPipelineRoot: ''
  ConMaxLifeTimeSec: '120'
  dbHost: mysql
  cacheNodeRestrictions: 'false'
  appName: pipeline
  autoUpdatePipelineDefaultVersion: 'true'
  mlmdDb: mlpipeline
  bucketName: dspipelines
  appVersion: 1.7.0
  cacheImage: registry.access.redhat.com/ubi8/ubi-minimal
  warning: |
    1. Do not use kubectl to edit this configmap, because some values are used
    during kustomize build. Instead, change the configmap and apply the entire
    kustomize manifests again.
    2. After updating the configmap, some deployments may need to be restarted
    until the changes take effect. A quick way to restart all deployments in a
    namespace: `kubectl rollout restart deployment -n <your-namespace>`.
  cronScheduleTimezone: UTC
  dbPort: '3306'
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: ds-pipeline-config
  namespace: redhat-ods-applications
data:
  artifact_endpoint: minio-service.minio.svc.cluster.local
  artifact_endpoint_scheme: 'http://'
  apply_tekton_custom_resource: 'true'
  archive_logs: 'false'
  track_artifacts: 'true'
  artifact_image: 'quay.io/opendatahub/ml-pipelines-artifact-manager:latest'
  terminate_status: Cancelled
  strip_eof: 'true'
  inject_default_script: 'true'
  artifact_bucket: dspipelines
  artifact_script: |-
    #!/usr/bin/env sh
    push_artifact() {
        if [ -f "$2" ]; then
            tar -cvzf $1.tgz $2
            aws s3 --endpoint ${ARTIFACT_ENDPOINT_SCHEME}${ARTIFACT_ENDPOINT} cp $1.tgz s3://$ARTIFACT_BUCKET/artifacts/$PIPELINERUN/$PIPELINETASK/$1.tgz
        else
            echo "$2 file does not exist. Skip artifact tracking for $1"
        fi
    }
    push_log() {
        cat /var/log/containers/$PODNAME*$NAMESPACE*step-main*.log > step-main.log
        push_artifact main-log step-main.log
    }
    strip_eof() {
        if [ -f "$2" ]; then
            awk 'NF' $2 | head -c -1 > $1_temp_save && cp $1_temp_save $2
        fi
    }
---
apiVersion: kfdef.apps.kubeflow.org/v1
kind: KfDef
metadata:
  name: rhods-beta-features
  namespace: redhat-ods-applications
spec:
  applications:
    - kustomizeConfig:
        overlays:
          - metadata-store-mariadb
          - integration-odhdashboard
          - ds-pipeline-ui
        repoRef:
          name: manifests
          path: data-science-pipelines
      name: data-science-pipelines
  repos:
    - name: manifests
      uri: 'https://github.com/red-hat-data-services/odh-manifests/tarball/master'
---
apiVersion: dashboard.opendatahub.io/v1
kind: OdhApplication
metadata:
  name: data-science-pipelines
  namespace: redhat-ods-applications
spec:
  enable:
    actionLabel: Enable
    description: >-
      Clicking enable will add a card to the Enabled page to access the Data
      Science Pipelines interface.


      Before enabling, be sure you have installed OpenShift Pipelines and have
      an S3 Object store configured
    title: Enable Data Science Pipelines
    validationConfigMap: ds-pipelines-dashboardtile-validation-result
  img: >-
    <svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg"
    viewBox="0 0 192
    145"><defs><style>.cls-1{fill:#e00;}</style></defs><title>RedHat-Logo-Hat-Color</title><path
    d="M157.77,62.61a14,14,0,0,1,.31,3.42c0,14.88-18.1,17.46-30.61,17.46C78.83,83.49,42.53,53.26,42.53,44a6.43,6.43,0,0,1,.22-1.94l-3.66,9.06a18.45,18.45,0,0,0-1.51,7.33c0,18.11,41,45.48,87.74,45.48,20.69,0,36.43-7.76,36.43-21.77,0-1.08,0-1.94-1.73-10.13Z"/><path
    class="cls-1"
    d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/></svg>
  getStartedLink: >-
    https://access.redhat.com/documentation/en-us/red_hat_openshift_data_science/1
  betaTitle: Data Science Pipelines (beta)
  route: ds-pipeline-ui
  internalRoute: ds-pipeline-ui
  displayName: Data Science Pipelines
  kfdefApplications: []
  support: Red Hat managed
  beta: true
  provider: Red Hat
  docsLink: >-
    https://access.redhat.com/documentation/en-us/red_hat_openshift_data_science/1
  quickStart: create-data-science-pipeline
  getStartedMarkDown: >-
    # Getting Started With Data Science Pipelines

    Below are the list of samples that are currently running end to end taking
    the compiled Tekton yaml and deploying on a Tekton cluster directly. If you
    are interested more in the larger list of pipelines samples we are testing
    for whether they can be 'compiled to Tekton' format, please [look at the
    corresponding status
    page](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/sdk/python/tests/README.md)

    [DSP Tekton User
    Guide](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/guides/kfp-user-guide)
    is a guideline for the possible ways to develop and consume Data Science
    Pipelines. It's recommended to go over at least one of the methods in the
    user guide before heading into the KFP Tekton Samples.

    ## Prerequisites 

    - Install [OpenShift Pipelines
    Operator](https://docs.openshift.com/container-platform/4.7/cicd/pipelines/installing-pipelines.html).
    Then connect the cluster to the current shell with `oc` 


    - Install
    [kfp-tekton](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/sdk/README.md)
    SDK

        ```
        # Set up the python virtual environment
        python3 -m venv .venv
        source .venv/bin/activate

        # Install the kfp-tekton SDK
        pip install kfp-tekton
        ```

    ## Samples

    - [MNIST End to End example with DSP
    components](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/e2e-mnist)


    - [Hyperparameter tuning using
    Katib](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/katib)


    - [Trusted AI Pipeline with AI Fairness 360 and Adversarial Robustness 360
    components](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/trusted-ai)


    - [Training and Serving Models with Watson Machine
    Learning](https://github.com/red-hat-data-service/sdata-science-pipelines/tree/master/samples/watson-train-serve#training-and-serving-models-with-watson-machine-learning)


    - [Lightweight python components
    example](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/lightweight-component)


    - [The flip-coin
    pipeline](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/flip-coin)


    - [Nested pipeline
    example](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/nested-pipeline)


    - [Pipeline with Nested
    loops](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/nested-loops)


    - [Using Tekton Custom Task on
    DSP](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/tekton-custom-task)


    - [The flip-coin pipeline using custom
    task](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/flip-coin-custom-task)


    - [Retrieve DSP run metadata using Kubernetes downstream
    API](https://github.com/red-hat-data-services/data-science-pipelines/tree/master/samples/k8s-downstream-api)
  description: >-
    Data Science Pipelines is a workflow platform with a focus on enabling
    Machine Learning operations such as Model development, experimentation,
    orchestration and automation.
  betaText: >-
    This beta application is available for early access prior to official
    release.
  category: Red Hat managed
