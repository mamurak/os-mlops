kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: triton
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/template-enabled: 'true'
    tags: 'triton,servingruntime'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: triton
      labels:
        name: modelmesh-serving-triton-2.x-SR
      annotations:
        maxLoadingConcurrency: '2'
    spec:
      supportedModelFormats:
        - name: keras
          version: '2'
          autoSelect: true
        - name: onnx
          version: '1'
          autoSelect: true
        - name: pytorch
          version: '1'
          autoSelect: true
        - name: tensorflow
          version: '1'
          autoSelect: true
        - name: tensorflow
          version: '2'
          autoSelect: true
        - name: tensorrt
          version: '7'
          autoSelect: true
      protocolVersions:
        - grpc-v2
      multiModel: true
      grpcEndpoint: 'port:8085'
      grpcDataEndpoint: 'port:8001'
      containers:
        - name: triton
          image: 'nvcr.io/nvidia/tritonserver:21.06.1-py3'
          command:
            - /bin/sh
          args:
            - '-c'
            - >-
              mkdir -p /models/_triton_models; chmod 777 /models/_triton_models;
              exec tritonserver "--model-repository=/models/_triton_models"
              "--model-control-mode=explicit" "--strict-model-config=false"
              "--strict-readiness=false" "--allow-http=true"
              "--allow-sagemaker=false" 
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: '5'
              memory: 1Gi
          livenessProbe:
            exec:
              command:
                - curl
                - '--fail'
                - '--silent'
                - '--show-error'
                - '--max-time'
                - '9'
                - 'http://localhost:8000/v2/health/live'
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 10
      builtInAdapter:
        serverType: triton
        runtimeManagementPort: 8001
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000
---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: mlserver
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/template-enabled: 'true'
    tags: 'mlserver,servingruntime'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: mlserver
      labels:
        name: modelmesh-serving-mlserver-1.x-SR
    spec:
      supportedModelFormats:
        - name: sklearn
          version: "0" # v0.23.1
          autoSelect: true
        - name: xgboost
          version: "1" # v1.1.1
          autoSelect: true
        - name: lightgbm
          version: "3" # v3.2.1
          autoSelect: true

      protocolVersions:
        - grpc-v2
      multiModel: true

      grpcEndpoint: "port:8085"
      grpcDataEndpoint: "port:8001"

      containers:
        - name: mlserver
          image: seldonio/mlserver:1.3.5
          env:
            - name: MLSERVER_MODELS_DIR
              value: "/models/_mlserver_models/"
            - name: MLSERVER_GRPC_PORT
              value: "8001"
            # default value for HTTP port is 8080 which conflicts with MMesh's
            # Litelinks port
            - name: MLSERVER_HTTP_PORT
              value: "8002"
            - name: MLSERVER_LOAD_MODELS_AT_STARTUP
              value: "false"
            # Set a dummy model name via environment so that MLServer doesn't
            # error on a RepositoryIndex call when no models exist
            - name: MLSERVER_MODEL_NAME
              value: dummy-model-fixme
            # Set server addr to localhost to ensure MLServer only listen inside the pod
            - name: MLSERVER_HOST
              value: "127.0.0.1"
            # Increase gRPC max message size to support larger payloads
            # Unlimited because it will be restricted at the model mesh layer
            - name: MLSERVER_GRPC_MAX_MESSAGE_LENGTH
              value: "-1"
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: "5"
              memory: 1Gi
      builtInAdapter:
        serverType: mlserver
        runtimeManagementPort: 8001
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000
---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: torchserve
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/template-enabled: 'true'
    tags: 'torchserve,servingruntime'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: torchserve
      labels:
        name: modelmesh-serving-torchserve-0.x-SR
    spec:
      supportedModelFormats:
        - name: pytorch-mar
          version: "0"
          autoSelect: true

      multiModel: true

      grpcEndpoint: "port:8085"
      grpcDataEndpoint: "port:7070"

      containers:
        - name: torchserve
          image: pytorch/torchserve:0.6.0-cpu
          args:
            # Adapter creates the config file; wait for it to exist before starting
            - while [ ! -e "$TS_CONFIG_FILE" ]; do echo "waiting for config file..."; sleep 1; done;
            - exec
            - torchserve
            - --start
            - --foreground
          env:
            - name: TS_CONFIG_FILE
              value: /models/_torchserve_models/mmconfig.properties
            # TBD, this may give better performance
            #- name: TS_PREFER_DIRECT_BUFFER
            #  value: true
            # Additional TS_ prefixed TorchServe config options may be added here
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: "5"
              memory: 1Gi
      builtInAdapter:
        serverType: torchserve
        runtimeManagementPort: 7071
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000
---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: vllm-4k
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/modelServingSupport: '["single"]'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    labels:
      opendatahub.io/dashboard: 'true'
    metadata:
      annotations:
        openshift.io/display-name: vLLM
      name: vllm-4k
    spec:
      builtInAdapter:
        modelLoadingTimeoutMillis: 90000
      containers:
        - args:
            - '--model'
            - /mnt/models/
            - '--download-dir'
            - /models-cache
            - '--port'
            - '8080'
            - '--dtype'
            - half
            - '--max-model-len'
            - '4096'
            - '--tensor_parallel_size'
            - '2'
          image: quay.io/rh-aiservices-bu/vllm-openai-ubi9:0.4.2
          name: kserve-container
          ports:
            - containerPort: 8080
              name: http1
              protocol: TCP
      multiModel: false
      supportedModelFormats:
        - autoSelect: true
          name: pytorch
---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: vllm-cpu
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/modelServingSupport: '["single"]'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    labels:
      opendatahub.io/dashboard: "true"
    metadata:
      annotations:
        openshift.io/display-name: "vLLM CPU"
      name: vllm-cpu
    spec:
      builtInAdapter:
        modelLoadingTimeoutMillis: 90000
      containers:
        - args:
            - --model
            - /mnt/models/
            - --download-dir
            - /models-cache
            - --port
            - "8080"
            - --max-model-len
            - "2048"
          image: quay.io/rh-aiservices-bu/vllm-cpu-openai-ubi9:0.1
          name: kserve-container
          ports:
            - containerPort: 8080
              name: http1
              protocol: TCP
      multiModel: false
      supportedModelFormats:
        - autoSelect: true
          name: pytorch
---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: kserve-mlserver
  namespace: redhat-ods-applications
  labels:
    app: odh-dashboard
    app.opendatahub.io/model-mesh: 'false'
    opendatahub.io/dashboard: 'true'
    opendatahub.io/ootb: 'false'
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      annotations:
        opendatahub.io/recommended-accelerators: ''
        openshift.io/display-name: ML Server
      labels:
        opendatahub.io/dashboard: 'true'
      name: kserve-mlserver
    spec:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '8080'
      containers:
        - name: kserve-container
          image: 'docker.io/seldonio/mlserver:1.5.0'
          env:
            - name: MLSERVER_MODEL_NAME
              value: '{{.Name}}'
            - name: MLSERVER_MODEL_IMPLEMENTATION
              value: mlserver_sklearn.SKLearnModel
            - name: MLSERVER_HTTP_PORT
              value: '8080'
            - name: MLSERVER_GRPC_PORT
              value: '9000'
            - name: MODELS_DIR
              value: /mnt/models
            - name: MLSERVER_MODEL_URI
              value: /mnt/models
          resources:
            requests:
              cpu: '1'
              memory: 2Gi
            limits:
              cpu: '1'
          memory: 2Gi
          ports:
            - containerPort: 8080
              protocol: TCP
      multiModel: false
      supportedModelFormats:
        - name: sklearn
          version: '0'
          autoSelect: true
        - name: sklearn
          version: '1'
          autoSelect: true
      protocolVersions:
        - v2
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: kserve-sdxl
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    openshift.io/display-name: "Kserve for SDXL"
    opendatahub.io/template-display-name: "Kserve for SDXL"
    opendatahub.io/template-name: kserve-sdxl
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  multiModel: false
  builtInAdapter:
    modelLoadingTimeoutMillis: 90000
  supportedModelFormats:
    - name: sdxl
      autoSelect: true
  containers:
    - name: kserve-container
      image: quay.io/rh-aiservices-bu/kserve-sdxl:0.0.1
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: SDXL-1.0
    serving.kserve.io/deploymentMode: RawDeployment
  name: sdxl
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: sdxl
      name: ''
      resources:
        limits:
          cpu: '6'
          memory: 24Gi
          nvidia.com/gpu: '1'
        requests:
          cpu: '4'
          memory: 16Gi
          nvidia.com/gpu: '1'
      runtime: kserve-sdxl
      args:
        - --single_file_model=/mnt/models/sd_xl_base_1.0.safetensors
        - --use_refiner=True
        - --refiner_single_file_model=/mnt/models/sd_xl_refiner_1.0.safetensors
        - --device=cuda
      storage:
        key: aws-connection-models
        path: /models/sdxl/
    tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Equal
        value: 'NVIDIA-A10G-SHARED'
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    openshift.io/display-name: vLLM NVIDIA GPU ServingRuntime for KServe
  name: vllm-cuda-runtime
  namespace: model
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: '8080'
  containers:
    - args:
        - '--port=8080'
        - '--model=/mnt/models'
        - '--served-model-name={{.Name}}'
        - '--tensor-parallel-size=4'
        - '--distributed-executor-backend=mp'
        - '--trust-remote-code'
        - '--max-model-len=4096'
      command:
        - python
        - '-m'
        - vllm.entrypoints.openai.api_server
      env:
        - name: HF_HOME
          value: /tmp/hf_home
      image: 'quay.io/modh/vllm@sha256:67822ff98835488ee34c25c2349b7b3d8c401928c45181b46ed533a9ccd7caba'
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: vLLM
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 2Gi
      name: shm
