{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Working with an LLM programmatically\n",
    "\n",
    "You have certainly interacted before with a Large Language Model (LLM) like ChatGPT. This is usually done through a UI or an application.\n",
    "\n",
    "In this Notebook, we are going to use Python to connect and query an LLM directly through its API. For this Lab we have selected the model **Mistral-7B Instruct v2**.(https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). This is a fully Open Source model (Apache 2.0 license), that although much lighter than other commercial or open source models is very capable, especially with the tasks we intend to use it for.\n",
    "\n",
    "This model has already been deployed on the Lab cluster because even if it's a smaller model, it still needs a GPU with 24GB of RAM to run..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "If you have selected the right workbench image to launch as per the Lab's instructions, you should already have all the needed libraries. If not uncomment the first line in the next cell to install all the right packages. We will then import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain (https://www.langchain.com/) is a framework for developing applications powered by language models. It will take care for us of all the boilerplate code we would have to manually write to properly query an LLM.\n",
    "\n",
    "We will start by creating an **llm** instance, defined by the location where the LLM API can be queried and some parameters that will be applied to the model. For example, `max_new_tokens` will instruct the model to answer with a maximum of 512 tokens (words or parts of words). `temperature`, set really low here, will instruct the model to stay truth-grounded, and not try to be too 'creative'. After all, we're not trying to write a fancy poem here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fcd00-0b77-4a7a-b138-fa7a8e4a1556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_server_url = 'REPLACE_ME'\n",
    "token = ''\n",
    "model_name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "We also need a **template** to be applied to every request we are sending to the model (the 'Prompt').\n",
    "\n",
    "When querying a model, you almost never want to send directly what the user has typed. On top of this entry, you need to give proper instructions to the model so that it knows how to handle it: what and how to answer, what NOT to answer, the tone it must use...\n",
    "\n",
    "Langchain allows us to now easily 'stitch' those elements together and create a **conversation** object that we will use to query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe2757-3bd2-4047-958d-c3d9dae74cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = VLLMOpenAI(\n",
    "    openai_api_key='NONE',\n",
    "    openai_api_base=f'{inference_server_url}/v1',\n",
    "    model_name=model_name or '/mnt/models/',\n",
    "    max_tokens=512,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    default_headers={'Authorization': f'Bearer {token}'},\n",
    ")\n",
    "\n",
    "template = '''<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, answer 'I don't know'.\n",
    "<</SYS>>\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "'''\n",
    "PROMPT = PromptTemplate(input_variables=['input'], template=template)\n",
    "\n",
    "conversation = LLMChain(llm=llm, prompt=PROMPT, verbose=False)\n",
    "\n",
    "query = 'What is Artificial Intelligence?'\n",
    "\n",
    "conversation.predict(input=query);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "## Basic HTTP queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a9b57-97a7-4d83-bfb1-a5bfc3a46625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, post\n",
    "\n",
    "response = get(f'{inference_server_url}/v1/models')\n",
    "try:\n",
    "    print(response.json())\n",
    "except Exception:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafeed92-7345-4400-9f1b-a1c5a451d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'messages': [{'content': 'Hi', 'role': 'user'}],\n",
    "    'model': model_name,\n",
    "}\n",
    "\n",
    "response = post(f'{inference_server_url}/v1/chat/completions', json=payload)\n",
    "try:\n",
    "    print(response.json())\n",
    "except Exception:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b51fb-2b2f-490e-9039-794d4b396e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
